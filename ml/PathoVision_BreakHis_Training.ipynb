{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd8042d8",
   "metadata": {},
   "source": [
    "# PathoVision: BreakHis Binary Classification (Benign vs Malignant)\n",
    "Academic support model for histopathology screening. Not a clinical diagnostic tool.\n",
    "\n",
    "**Goals**\n",
    "- Binary classification (Benign/Malignant)\n",
    "- ResNet50 transfer learning\n",
    "- Grad-CAM explainability\n",
    "- Kaggle/Colab ready\n",
    "- Exportable for backend inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ff2ff0",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0b01d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, roc_curve, auc\n",
    ")\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666558a3",
   "metadata": {},
   "source": [
    "## 2. Dataset Setup (BreakHis)\n",
    "Set the dataset path below. Example for Kaggle: `/kaggle/input/breakhis`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4118bd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = '/kaggle/input/breakhis'  # change if needed\n",
    "\n",
    "# Expected BreakHis path structure (one possible layout):\n",
    "# /BreaKHis_v1/histology_slides/breast/benign/SOB/.../40X/*.png\n",
    "# /BreaKHis_v1/histology_slides/breast/malignant/SOB/.../40X/*.png\n",
    "\n",
    "benign_paths = glob(os.path.join(DATA_ROOT, '**', 'benign', '**', '*.png'), recursive=True)\n",
    "malignant_paths = glob(os.path.join(DATA_ROOT, '**', 'malignant', '**', '*.png'), recursive=True)\n",
    "\n",
    "print('Benign images:', len(benign_paths))\n",
    "print('Malignant images:', len(malignant_paths))\n",
    "\n",
    "all_paths = benign_paths + malignant_paths\n",
    "all_labels = [0] * len(benign_paths) + [1] * len(malignant_paths)\n",
    "\n",
    "# Build dataframe for easy splits\n",
    "df = pd.DataFrame({'path': all_paths, 'label': all_labels})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcd55df",
   "metadata": {},
   "source": [
    "## 3. Train/Val/Test Split (70/15/15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96347f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, temp_df = train_test_split(\n",
    "    df, test_size=0.30, random_state=SEED, stratify=df['label']\n",
    ")\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df, test_size=0.50, random_state=SEED, stratify=temp_df['label']\n",
    ")\n",
    "\n",
    "print('Train:', len(train_df), 'Val:', len(val_df), 'Test:', len(test_df))\n",
    "train_df['label'].value_counts(), val_df['label'].value_counts(), test_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c38d6bf",
   "metadata": {},
   "source": [
    "## 4. Transforms and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0f1a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "\n",
    "train_tfms = T.Compose([\n",
    "    T.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    T.RandomHorizontalFlip(p=0.5),\n",
    "    T.RandomRotation(degrees=10),\n",
    "    T.ColorJitter(brightness=0.1),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_tfms = T.Compose([\n",
    "    T.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "class BreakHisDataset(Dataset):\n",
    "    def __init__(self, df, transforms=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img = Image.open(row['path']).convert('RGB')\n",
    "        if self.transforms:\n",
    "            img = self.transforms(img)\n",
    "        label = int(row['label'])\n",
    "        return img, label\n",
    "\n",
    "train_ds = BreakHisDataset(train_df, transforms=train_tfms)\n",
    "val_ds = BreakHisDataset(val_df, transforms=val_tfms)\n",
    "test_ds = BreakHisDataset(test_df, transforms=val_tfms)\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3910c27",
   "metadata": {},
   "source": [
    "## 5. Model (ResNet50 Transfer Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6a3ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import ResNet50_Weights\n",
    "\n",
    "model = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
    "\n",
    "# Freeze early layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze layer4 and fc for fine-tuning\n",
    "for param in model.layer4.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8608052c",
   "metadata": {},
   "source": [
    "## 6. Training Loop with Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa2bf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0.001):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    for images, labels in tqdm(loader, leave=False):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader, leave=False):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "EPOCHS = 15\n",
    "early_stopping = EarlyStopping(patience=5)\n",
    "\n",
    "history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "    val_loss, val_acc = evaluate(model, val_loader, criterion)\n",
    "\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_acc'].append(val_acc)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{EPOCHS} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}')\n",
    "\n",
    "    early_stopping(val_loss)\n",
    "    if early_stopping.early_stop:\n",
    "        print('Early stopping triggered')\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd47b674",
   "metadata": {},
   "source": [
    "## 7. Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5906e054",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history['train_loss'], label='Train Loss')\n",
    "plt.plot(history['val_loss'], label='Val Loss')\n",
    "plt.title('Loss vs Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history['train_acc'], label='Train Acc')\n",
    "plt.plot(history['val_acc'], label='Val Acc')\n",
    "plt.title('Accuracy vs Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0018ab73",
   "metadata": {},
   "source": [
    "## 8. Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a11b3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "all_probs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_loader):\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        preds = torch.argmax(probs, dim=1)\n",
    "\n",
    "        all_labels.extend(labels.numpy())\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_probs.extend(probs[:, 1].cpu().numpy())\n",
    "\n",
    "acc = accuracy_score(all_labels, all_preds)\n",
    "prec = precision_score(all_labels, all_preds)\n",
    "rec = recall_score(all_labels, all_preds)\n",
    "f1 = f1_score(all_labels, all_preds)\n",
    "\n",
    "print(f'Test Accuracy: {acc:.4f}')\n",
    "print(f'Precision: {prec:.4f}')\n",
    "print(f'Recall: {rec:.4f}')\n",
    "print(f'F1 Score: {f1:.4f}')\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(5, 4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Benign', 'Malignant'], yticklabels=['Benign', 'Malignant'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "\n",
    "fpr, tpr, _ = roc_curve(all_labels, all_probs)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.4f}')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee98559",
   "metadata": {},
   "source": [
    "## 9. Grad-CAM Explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a1f481",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "class GradCAM:\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "        self._register_hooks()\n",
    "\n",
    "    def _register_hooks(self):\n",
    "        def forward_hook(module, input, output):\n",
    "            self.activations = output\n",
    "\n",
    "        def backward_hook(module, grad_input, grad_output):\n",
    "            self.gradients = grad_output[0]\n",
    "\n",
    "        self.target_layer.register_forward_hook(forward_hook)\n",
    "        self.target_layer.register_full_backward_hook(backward_hook)\n",
    "\n",
    "    def generate(self, input_tensor, class_idx=None):\n",
    "        self.model.eval()\n",
    "        output = self.model(input_tensor)\n",
    "        if class_idx is None:\n",
    "            class_idx = output.argmax(dim=1).item()\n",
    "\n",
    "        self.model.zero_grad()\n",
    "        output[0, class_idx].backward()\n",
    "\n",
    "        gradients = self.gradients[0]\n",
    "        activations = self.activations[0]\n",
    "        weights = torch.mean(gradients, dim=(1, 2))\n",
    "\n",
    "        cam = torch.zeros(activations.shape[1:], dtype=torch.float32).to(device)\n",
    "        for i, w in enumerate(weights):\n",
    "            cam += w * activations[i]\n",
    "\n",
    "        cam = torch.relu(cam)\n",
    "        cam = cam - cam.min()\n",
    "        cam = cam / (cam.max() + 1e-8)\n",
    "        cam = cam.detach().cpu().numpy()\n",
    "        return cam\n",
    "\n",
    "def overlay_heatmap(image_path, cam, output_path='heatmap.png'):\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    cam = cv2.resize(cam, (img.shape[1], img.shape[0]))\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)\n",
    "    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
    "    overlay = cv2.addWeighted(img, 0.6, heatmap, 0.4, 0)\n",
    "    cv2.imwrite(output_path, cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR))\n",
    "    return output_path\n",
    "\n",
    "gradcam = GradCAM(model, model.layer4[-1])\n",
    "\n",
    "# Example usage\n",
    "sample_path = test_df.iloc[0]['path']\n",
    "sample_img = Image.open(sample_path).convert('RGB')\n",
    "input_tensor = val_tfms(sample_img).unsqueeze(0).to(device)\n",
    "cam = gradcam.generate(input_tensor)\n",
    "os.makedirs('heatmaps', exist_ok=True)\n",
    "heatmap_path = overlay_heatmap(sample_path, cam, output_path='heatmaps/sample_01.png')\n",
    "print('Saved heatmap to:', heatmap_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba3edf7",
   "metadata": {},
   "source": [
    "## 10. Inference Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ae1cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = {0: 'Benign', 1: 'Malignant'}\n",
    "\n",
    "def predict_image(image_path, model, gradcam):\n",
    "    model.eval()\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    input_tensor = val_tfms(image).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        probs = torch.softmax(output, dim=1).cpu().numpy()[0]\n",
    "\n",
    "    pred_idx = int(np.argmax(probs))\n",
    "    confidence = float(probs[pred_idx])\n",
    "\n",
    "    cam = gradcam.generate(input_tensor, class_idx=pred_idx)\n",
    "    os.makedirs('heatmaps', exist_ok=True)\n",
    "    heatmap_path = overlay_heatmap(image_path, cam, output_path=f'heatmaps/{os.path.basename(image_path)}')\n",
    "\n",
    "    return {\n",
    "        'prediction': class_names[pred_idx],\n",
    "        'confidence': confidence,\n",
    "        'probabilities': {\n",
    "            'benign': float(probs[0]),\n",
    "            'malignant': float(probs[1])\n",
    "        },\n",
    "        'heatmap_path': heatmap_path\n",
    "    }\n",
    "\n",
    "# Example\n",
    "result = predict_image(sample_path, model, gradcam)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65d8198",
   "metadata": {},
   "source": [
    "## 11. Export Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29dc9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('models', exist_ok=True)\n",
    "torch.save(model.state_dict(), 'models/pathovision_resnet50.pt')\n",
    "print('Model saved to models/pathovision_resnet50.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9305254e",
   "metadata": {},
   "source": [
    "## 12. Simple FastAPI Inference Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5a9d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as app.py for FastAPI usage\n",
    "fastapi_example = r'''\n",
    "from fastapi import FastAPI, File, UploadFile\n",
    "import uvicorn\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Load model\n",
    "model = ...  # load model and weights\n",
    "model.eval()\n",
    "\n",
    "val_tfms = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "@app.post('/predict')\n",
    "async def predict(file: UploadFile = File(...)):\n",
    "    image = Image.open(file.file).convert('RGB')\n",
    "    input_tensor = val_tfms(image).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_tensor)\n",
    "        probs = torch.softmax(outputs, dim=1).cpu().numpy()[0]\n",
    "    pred_idx = int(np.argmax(probs))\n",
    "    return {\n",
    "        'prediction': 'Malignant' if pred_idx == 1 else 'Benign',\n",
    "        'confidence': float(probs[pred_idx]),\n",
    "        'probabilities': {\n",
    "            'benign': float(probs[0]),\n",
    "            'malignant': float(probs[1])\n",
    "        },\n",
    "        'heatmap_path': 'heatmaps/sample.png'\n",
    "    }\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    uvicorn.run(app, host='0.0.0.0', port=8000)\n",
    "'''\n",
    "print(fastapi_example)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
